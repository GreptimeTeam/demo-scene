{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adaad934-d934-4441-8f4f-9326326bcd6c",
   "metadata": {},
   "source": [
    "# Prerequisite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f932b5-98d6-4ba4-aed1-d7b2b0af02b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch transformers ipyplot datasets pymysql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d253f7a3-4388-46ca-a5b2-adbf5fe4d175",
   "metadata": {},
   "source": [
    "# Init Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb426d5d-324b-4b01-8a66-15ac36228493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "def get_connection():\n",
    "    connection = pymysql.connect(\n",
    "        host = \"greptimedb\",\n",
    "        port = 4002,\n",
    "        user = \"root\",\n",
    "        database = \"public\",\n",
    "    )\n",
    "    return connection\n",
    "c = get_connection()\n",
    "cursor = c.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545200a0-2c25-480e-b8e7-8ac98390da5d",
   "metadata": {},
   "source": [
    "# Create Table with Vector Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f36a5d-0dca-473a-bdaf-292e1fe907f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS embedded_images_large(\n",
    "    ts TIMESTAMP TIME INDEX DEFAULT CURRENT_TIMESTAMP,\n",
    "    image_id INT PRIMARY KEY,\n",
    "    embedding VECTOR(512));\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce46812-ed57-4eab-a29a-29b6efac204d",
   "metadata": {},
   "source": [
    "# Prepare Model and Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3861bab-96e6-4b44-bd87-0f8fdc5a0d5e",
   "metadata": {},
   "source": [
    "Note that loading the model may take minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67c9fa0-9e71-4b16-adc8-920e9d45e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "import datasets\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "imagenet_datasets = datasets.load_dataset('zh-plus/tiny-imagenet', split='train')\n",
    "\n",
    "def encode_images_to_embeddings(images):\n",
    "    # accept a list of images and return the image embeddings\n",
    "    with torch.no_grad():\n",
    "        inputs = processor(images=images, return_tensors=\"pt\")\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "        return image_features.cpu().detach().numpy()\n",
    "\n",
    "def encode_text_to_embedding(text):\n",
    "    # accept a text and return the text embedding\n",
    "    with torch.no_grad():\n",
    "        inputs = processor(text=text, return_tensors=\"pt\")\n",
    "        text_features = model.get_text_features(**inputs)\n",
    "        return text_features.cpu().detach().numpy()[0]\n",
    "\n",
    "\n",
    "def embedding_s(embedding):\n",
    "    return f\"[{','.join(map(str, embedding))}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dd8f57-5139-4d7c-b7d2-35894af316ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_datasets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6094057-7f14-4844-842a-93f61ddc682c",
   "metadata": {},
   "source": [
    "# Store Embeddings of Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941646ff-1260-45a4-af0f-dedd712a9e9e",
   "metadata": {},
   "source": [
    "Note that calculating the embedding of 100,000 images may take more than half an hour, so you can prepare the data in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc59827-fede-4070-ac80-8e15afaaff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_images = [i['image'] for i in imagenet_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55c46d0-593f-433c-84bf-35d93a35549f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def insert_images(images_embedding, begin_i):\n",
    "    for i in range(len(images_embedding)):\n",
    "        embedding = embedding_s(images_embedding[i])\n",
    "        cursor.execute(f\"\"\"\n",
    "INSERT INTO embedded_images_large VALUES (DEFAULT, {i+begin_i}, '{embedding}');\n",
    "        \"\"\");\n",
    "\n",
    "batch_size = 1000\n",
    "batch = int(len(imagenet_images) / batch_size)\n",
    "\n",
    "for b_i in range(batch):\n",
    "    image_batch = imagenet_images[b_i*batch_size:b_i*batch_size+batch_size]\n",
    "    images_embedding = encode_images_to_embeddings(image_batch)\n",
    "    insert_images(images_embedding, b_i*batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cb9181-ec97-4a68-8e06-6a2e161b4308",
   "metadata": {},
   "source": [
    "# Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0171a01f-59ea-478d-8d2a-8c9158405e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import ipyplot\n",
    "import time\n",
    "\n",
    "def search(query, k):\n",
    "    query_embedding = embedding_s(encode_text_to_embedding(query))\n",
    "    start = time.time()\n",
    "    cursor.execute(f\"\"\"\n",
    "SELECT image_id, cos_distance(embedding, '{query_embedding}') AS distance\n",
    "FROM embedded_images_large\n",
    "ORDER BY distance\n",
    "LIMIT {k};\n",
    "    \"\"\");\n",
    "    res = cursor.fetchall()\n",
    "    print(f\"Time taken: {time.time() - start}\")\n",
    "    return res\n",
    "\n",
    "res = search(\"fire\", 10)\n",
    "similar_images = []\n",
    "similarities = []\n",
    "\n",
    "for image_id, d in res:\n",
    "    image = imagenet_images[image_id]\n",
    "    if image.mode == 'L':\n",
    "        image = image.convert('RGB')\n",
    "    np_image = np.array(image)\n",
    "    similar_images.append(np_image)\n",
    "    similarities.append(round(1 - d, 3))\n",
    "\n",
    "ipyplot.plot_images(similar_images, labels=similarities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
